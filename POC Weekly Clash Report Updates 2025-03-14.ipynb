{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c937d2-4ed3-4803-b685-fa9a72cbbfe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43aec434-2f15-422d-942e-3ba33a4bab4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Get the latest SC Weekly Clash Report from the sharepoint location - use local file if sharepoint approach no available\n",
    "# local path is : /Workspace/Users/paul.schmidt@mnscorp.net/tmp/WeeklyClash_20241127.xlsx\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the file in DBFS\n",
    "file_path = \"/Workspace/Users/paul.schmidt@mnscorp.net/tmp/WeeklyClash_20241127.xlsx\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "  print(\"File found! Processing...\")\n",
    "  \n",
    "  # Read Excel file\n",
    "  clashdf = pd.read_excel(file_path, sheet_name=\"Sheet1\", header=0)  # Change sheet_name if needed\n",
    "  \n",
    "  # Check for missing column names\n",
    "  missing_headers = [col for col in clashdf.columns if \"Unnamed\" in str(col) or pd.isna(col)]\n",
    "  if missing_headers:\n",
    "    print(\"Missing headers found:\", missing_headers)\n",
    "  else:\n",
    "    print(\"All headers are valid!\")\n",
    "else:\n",
    "    print(\"File not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c609d35-af25-4e99-b099-7c5793176a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Display DataFrame in Databricks\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if clashdf.empty:\n",
    "    print(\"DataFrame is empty. Skipping Spark conversion.\")\n",
    "else:\n",
    "  # Replace missing headers with default names (col_0, col_1, etc.)\n",
    "  clashdf.columns = [f\"col_{i}\" if \"Unnamed\" in str(col) or pd.isna(col) else col for i, col in enumerate(clashdf.columns)]\n",
    "\n",
    "  # Convert Pandas DataFrame to Spark DataFrame\n",
    "  schema = StructType([\n",
    "    StructField(\"Property Name / Ref\", StringType(), True),\n",
    "    StructField(\"Project Ref\", StringType(), True),\n",
    "    StructField(\"Programme\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Source\", StringType(), True),\n",
    "    StructField(\"Principal Contractor\", StringType(), True),\n",
    "    StructField(\"Contact Name\", StringType(), True),\n",
    "    StructField(\"Contact Email\", StringType(), True),\n",
    "    StructField(\"SOS\", StringType(), True),\n",
    "    StructField(\"PC\", StringType(), True),\n",
    "    StructField(\"Launch\", StringType(), True),\n",
    "    StructField(\"Day / Night Work\", StringType(), True),\n",
    "    StructField(\"Clash\", StringType(), True),\n",
    "    StructField(\"col_14\", StringType(), True),\n",
    "    StructField(\"RAG\", StringType(), True),\n",
    "    StructField(\"Needs to be reviewed by\", StringType(), True),\n",
    "    StructField(\"Reviewed Y/N\", StringType(), True),\n",
    "    StructField(\"Comments\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "  # Convert to Spark DataFrame using schema\n",
    "  spark_df = spark.createDataFrame(clashdf.to_dict(orient=\"records\"), schema=schema)\n",
    "\n",
    "  # Replace NaN with empty string for string columns and null for numeric columns\n",
    "  spark_df = spark_df.select(\n",
    "    *[\n",
    "        F.when(F.col(c).isNull() | (F.col(c) == \"NaN\"), \"\").otherwise(F.col(c)).alias(c)  \n",
    "        for c in spark_df.columns\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  # Display Spark DataFrame\n",
    "  display(spark_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c8677c-5e43-4cd0-b7f6-0c8b76c01322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Write the Spark DataFrame into a Temp Table\n",
    "# spark_df.createOrReplaceTempView(\"curr_clash_data\")\n",
    "\n",
    "# alter the column names to something more db like\n",
    "spark_df.selectExpr(\n",
    "  \"`Property Name / Ref` AS property_name\",\n",
    "  \"`Project Ref` AS property_ref\",\n",
    "  \"Programme AS programme\",\n",
    "  \"Status AS status\",\n",
    "  \"Description AS description\",\n",
    "  \"Source AS source\",\n",
    "  \"`Principal Contractor` AS principal_contractor\",\n",
    "  \"`Contact Name` AS contact_name\",\n",
    "  \"`Contact Email` AS contact_email\",\n",
    "  \"SOS AS sos\",\n",
    "  \"PC AS pc\",\n",
    "  \"Launch AS launch\",\n",
    "  \"`Day / Night Work` AS day_night_work\",\n",
    "  \"Clash AS clash\",\n",
    "  \"col_14 AS comments\"\n",
    ").createOrReplaceTempView(\"curr_clash_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d688de-404a-49f3-b862-918337d5b778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM curr_clash_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be68764-526b-4c91-b596-0f7ea9518312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Get last week's Amended Weekly Clash Report from the sharepoint location - use local file if sharepoint approach no available\n",
    "# local path is : /Workspace/Users/paul.schmidt@mnscorp.net/tmp/AmendedWeeklyClash_20241120.xlsx\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the file in DBFS\n",
    "file_path = \"/Workspace/Users/paul.schmidt@mnscorp.net/tmp/AmendedWeeklyClash_20241120.xlsx\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "  print(\"File found! Processing...\")\n",
    "  \n",
    "  # Read Excel file\n",
    "  amendclashdf = pd.read_excel(file_path, sheet_name=\"21.11.24\", header=0)  # Change sheet_name if needed\n",
    "  \n",
    "  # Check for missing column names\n",
    "  missing_headers = [col for col in amendclashdf.columns if \"Unnamed\" in str(col) or pd.isna(col)]\n",
    "  if missing_headers:\n",
    "    print(\"Missing headers found:\", missing_headers)\n",
    "  else:\n",
    "    print(\"All headers are valid!\")\n",
    "else:\n",
    "    print(\"File not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35f5f18-8566-447f-872b-5752fa68c068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Display DataFrame in Databricks\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if amendclashdf.empty:\n",
    "    print(\"DataFrame is empty. Skipping Spark conversion.\")\n",
    "else:\n",
    "  # Replace missing headers with default names (col_0, col_1, etc.)\n",
    "  amendclashdf.columns = [f\"col_{i}\" if \"Unnamed\" in str(col) or pd.isna(col) else col for i, col in enumerate(amendclashdf.columns)]\n",
    "\n",
    "  # Convert Pandas DataFrame to Spark DataFrame\n",
    "  schema = StructType([\n",
    "    StructField(\"Project Ref\", StringType(), True),\n",
    "    StructField(\"Property Name / Ref\", StringType(), True),\n",
    "    StructField(\"RAG\", StringType(), True),\n",
    "    StructField(\"Programme\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Principal Contractor\", StringType(), True),\n",
    "    StructField(\"Contact Name\", StringType(), True),\n",
    "    StructField(\"Contact Email\", StringType(), True),\n",
    "    StructField(\"SOS\", StringType(), True),\n",
    "    StructField(\"PC\", StringType(), True),\n",
    "    StructField(\"Launch\", StringType(), True),\n",
    "    StructField(\"Clash\", StringType(), True),\n",
    "    StructField(\"Needs to be reviewed by\", StringType(), True),\n",
    "    StructField(\"Reviewed Y/N\", StringType(), True),\n",
    "    StructField(\"Comments\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "  # Convert to Spark DataFrame using schema\n",
    "  amendspark_df = spark.createDataFrame(amendclashdf.to_dict(orient=\"records\"), schema=schema)\n",
    "\n",
    "  # Replace NaN with empty string for string columns and null for numeric columns\n",
    "  amendspark_df = amendspark_df.select(\n",
    "    *[\n",
    "        F.when(F.col(c).isNull() | (F.col(c) == \"NaN\"), \"\").otherwise(F.col(c)).alias(c) \n",
    "        for c in amendspark_df.columns\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  # Display Spark DataFrame\n",
    "  display(amendspark_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa37e2f-1241-47cf-b597-3aaf9d7cb8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Write the Amended Spark DataFrame into a Temp Table\n",
    "# amendspark_df.createOrReplaceTempView(\"prev_clash_data\")\n",
    "\n",
    "# alter the column names to something more db like\n",
    "amendspark_df.selectExpr(\n",
    "  \"`Project Ref` AS property_ref\",\n",
    "  \"`Property Name / Ref` AS property_name\",\n",
    "  \"RAG AS rag\",\n",
    "  \"Programme AS programme\",\n",
    "  \"Status AS status\",\n",
    "  \"Description AS description\",\n",
    "  \"`Principal Contractor` AS principal_contractor\",\n",
    "  \"`Contact Name` AS contact_name\",\n",
    "  \"`Contact Email` AS contact_email\",\n",
    "  \"SOS AS sos\",\n",
    "  \"PC AS pc\",\n",
    "  \"Launch AS launch\",\n",
    "  \"Clash AS clash\",\n",
    "  \"`Needs to be reviewed by` AS needs_to_be_reviewed_by\",\n",
    "  \"`Reviewed Y/N` AS reviewed_y_n\",\n",
    "  \"Comments AS comments\"\n",
    ").createOrReplaceTempView(\"prev_clash_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa3be29-78cb-4e03-bd4b-ce28a2fc5175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM prev_clash_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d467d8-6413-4c1c-a412-9a2ed9756c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Combine some date from the prev table into the curr table and insert the new data into a new combined table\n",
    "CREATE OR REPLACE TEMP VIEW new_clash_data AS\n",
    "SELECT \n",
    "    curr.property_ref,\n",
    "    curr.property_name,\n",
    "    CASE \n",
    "        WHEN prev.rag IS NOT NULL THEN prev.rag \n",
    "        ELSE ''\n",
    "    END AS rag,\n",
    "    curr.programme,\n",
    "    curr.status,\n",
    "    curr.description,\n",
    "    curr.principal_contractor,\n",
    "    curr.contact_name,\n",
    "    curr.contact_email,\n",
    "    curr.sos,\n",
    "    curr.pc,\n",
    "    curr.launch,\n",
    "    curr.clash,\n",
    "    CASE \n",
    "        WHEN prev.needs_to_be_reviewed_by IS NOT NULL THEN prev.needs_to_be_reviewed_by \n",
    "        ELSE ''\n",
    "    END AS needs_to_be_reviewed_by,\n",
    "    CASE \n",
    "        WHEN prev.reviewed_y_n IS NOT NULL THEN prev.reviewed_y_n \n",
    "        ELSE ''\n",
    "    END AS reviewed_y_n,\n",
    "    CASE \n",
    "        WHEN prev.comments IS NOT NULL THEN prev.comments \n",
    "        ELSE ''\n",
    "    END AS comments\n",
    "FROM curr_clash_data curr\n",
    "LEFT JOIN prev_clash_data prev \n",
    "ON prev.property_ref = curr.property_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae0bc30-45f7-405d-9a9b-a3cf73b9f188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM new_clash_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59586c63-2efe-4f6c-80af-8bcb5ce8270e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Load the data from the new_clash_data view into a Spark DataFrame\n",
    "newspark_df = spark.sql(\"SELECT * FROM new_clash_data\")\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "newpandas_df = newspark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0853d6-cbce-4f1e-acbf-51e0e2fe08ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Create a new excel file with row highlighting based on the rag value\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# Create a workbook and add a sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"NewClash\"\n",
    "\n",
    "# Write Pandas DataFrame to the Excel sheet\n",
    "for r in dataframe_to_rows(newpandas_df, index=False, header=True):\n",
    "    ws.append(r)\n",
    "\n",
    "# Define the highlight color for the header (grey)\n",
    "header_fill = PatternFill(start_color=\"D3D3D3\", end_color=\"D3D3D3\", fill_type=\"solid\")\n",
    "bold_font = Font(bold=True)\n",
    "\n",
    "# Apply header formatting: Grey fill and bold font\n",
    "for cell in ws[1]:\n",
    "    cell.fill = header_fill  # Set grey color\n",
    "    cell.font = bold_font    # Set bold font\n",
    "\n",
    "# Rename the headers (change as needed)\n",
    "new_headers = ['Project Ref', 'Property Name / Ref', 'RAG', 'Programme', 'Status', 'Description', \n",
    "               'Principal Contractor', 'Contact Name', 'Contact Email', 'SOS', 'PC', 'Launch', \n",
    "               'Clash', 'Needs to be reviewed by', 'Reviewed Y/N', 'Comments']\n",
    "for col_num, new_header in enumerate(new_headers, start=1):\n",
    "    ws.cell(row=1, column=col_num).value = new_header\n",
    "\n",
    "# Define the highlight color (for example, light yellow)\n",
    "highlight_red = PatternFill(start_color=\"FCE4D6\", end_color=\"FCE4D6\", fill_type=\"solid\")\n",
    "highlight_amber = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
    "highlight_green = PatternFill(start_color=\"E2EFDA\", end_color=\"E2EFDA\", fill_type=\"solid\")\n",
    "highlight_none = PatternFill(start_color=\"FFF2CC\", end_color=\"FFF2CC\", fill_type=\"solid\")\n",
    "\n",
    "# Apply row highlighting based on a column value (e.g., 'status' column)\n",
    "for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):\n",
    "    if row[2].value == 'R':  # Check for a specific condition (e.g., 'R' in the third column)\n",
    "        for cell in row:  # Highlight every cell in the row\n",
    "            cell.fill = highlight_red\n",
    "    if row[2].value == 'A':  # Check for a specific condition (e.g., 'A' in the third column)\n",
    "        for cell in row:  # Highlight every cell in the row\n",
    "            cell.fill = highlight_amber\n",
    "    if row[2].value == 'G':  # Check for a specific condition (e.g., 'G' in the third column)\n",
    "        for cell in row:  # Highlight every cell in the row\n",
    "            cell.fill = highlight_green\n",
    "\n",
    "# Make the header row filterable\n",
    "ws.auto_filter.ref = ws.dimensions  # Enable autofilter for all columns\n",
    "\n",
    "# Set the height of the header row (Row 1)\n",
    "ws.row_dimensions[1].height = 30  # Adjust the height to your preference\n",
    "\n",
    "# Auto resize columns to fit data\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    column = col[0].column_letter  # Get the column name\n",
    "    for cell in col:\n",
    "        try:\n",
    "            if len(str(cell.value)) > max_length:\n",
    "                max_length = len(cell.value)\n",
    "        except:\n",
    "            pass\n",
    "    adjusted_width = (max_length + 2)  # Add some extra space\n",
    "    ws.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "# Set the width of some columns\n",
    "ws.column_dimensions['B'].width = 30  # Property Name / Ref\n",
    "ws.column_dimensions['C'].width = 10  # RAG\n",
    "ws.column_dimensions['D'].width = 30  # Programme\n",
    "ws.column_dimensions['F'].width = 60  # Description\n",
    "ws.column_dimensions['G'].width = 30  # principal Contractor\n",
    "ws.column_dimensions['H'].width = 30  # Contact Name\n",
    "ws.column_dimensions['I'].width = 30  # Contact Email\n",
    "ws.column_dimensions['M'].width = 10  # Clash\n",
    "ws.column_dimensions['O'].width = 15  # Reviewed Y/N\n",
    "ws.column_dimensions['O'].width = 50  # Comments\n",
    "\n",
    "# Hide specific columns (Column E Status / H Contact Name / I - Contact Email)\n",
    "ws.column_dimensions['E'].hidden = True\n",
    "ws.column_dimensions['H'].hidden = True\n",
    "ws.column_dimensions['I'].hidden = True\n",
    "\n",
    "# Save the Excel file\n",
    "output_path = '/Workspace/Users/paul.schmidt@mnscorp.net/tmp/NewWeeklyClash_20241127_01.xlsx'\n",
    "wb.save(output_path)\n",
    "\n",
    "# Display the file path\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4872ca-11e5-4d36-90c4-bf86b2008393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Monday.com API token\n",
    "API_KEY = \"eyJhbGciOiJIUzI1NiJ9.eyJ0aWQiOjM3Njk3OTg2MCwiYWFpIjoxMSwidWlkIjo1NTM4MTU1NCwiaWFkIjoiMjAyNC0wNi0yNlQwODo1MToyNC4wMDBaIiwicGVyIjoibWU6d3JpdGUiLCJhY3RpZCI6MTYyODE4OTcsInJnbiI6ImV1YzEifQ.npWkHCmQ-gjUgVrrahTDW9ofu8x4ATS4O0YPGmVdKRQ\"\n",
    "URL = \"https://api.monday.com/v2\"\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": API_KEY,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Define your specific board ID\n",
    "BOARD_ID = 1854342363\n",
    "\n",
    "# Initialize variables for pagination\n",
    "all_items = []\n",
    "cursor = None  # Start without a cursor\n",
    "\n",
    "# Collect all dynamic column names for the schema\n",
    "dynamic_columns = set()  # Using a set to avoid duplicates\n",
    "\n",
    "while True:\n",
    "    # Define GraphQL query correctly formatted as a string\n",
    "    query = {\n",
    "        \"query\": \"\"\"\n",
    "        {\n",
    "          boards (ids: [%s]) {\n",
    "            id\n",
    "            name\n",
    "            items_page (limit: 100, cursor: %s) {\n",
    "              cursor\n",
    "              items {\n",
    "                id\n",
    "                name\n",
    "                column_values {\n",
    "                  id\n",
    "                  text\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        \"\"\" % (BOARD_ID, f'\"{cursor}\"' if cursor else \"null\")\n",
    "    }\n",
    "\n",
    "    # Make API request\n",
    "    response = requests.post(URL, headers=headers, json=query)\n",
    "\n",
    "    # Check if the response is valid\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "        break  # Exit if API call fails\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if data is available\n",
    "    if \"data\" not in data or not data[\"data\"][\"boards\"]:\n",
    "        print(\"No data received from Monday.com API.\")\n",
    "        break\n",
    "\n",
    "    # Extract board data\n",
    "    board_data = data[\"data\"][\"boards\"][0]\n",
    "    board_id = board_data[\"id\"]\n",
    "    board_name = board_data[\"name\"]\n",
    "    items_page = board_data[\"items_page\"]\n",
    "\n",
    "    # Process items\n",
    "    for item in items_page[\"items\"]:\n",
    "        row = {\"Board ID\": board_id, \"Board Name\": board_name, \"Item ID\": item[\"id\"], \"Item Name\": item[\"name\"]}\n",
    "        # Add specific columns\n",
    "        #for col in item[\"column_values\"]:\n",
    "        #    print(\"Column Value:\", col)  # Inspect the column_value structure\n",
    "        #    if col.get(\"id\") == \"status_15__1\":\n",
    "        #      ## col_status_15__1 = col.get(\"id\")\n",
    "        #      col_status_15__1 = \"Status\"\n",
    "        #      col_value_status_15__1 = col.get(\"text\")\n",
    "        #      row[col_status_15__1] = col_value_status_15__1\n",
    "        #      dynamic_columns.add(col_status_15__1)\n",
    "        #      print(\"Specific Col ID\", col.get(\"id\"))\n",
    "        #      print(\"Specific Col Value\", col.get(\"text\"))\n",
    "\n",
    "        # Add all columns dynamically\n",
    "        for col in item[\"column_values\"]:\n",
    "          print(\"Column Value:\", col)  # Inspect the column_value structure\n",
    "          column_id = col.get(\"id\")\n",
    "          column_text = col.get(\"text\")\n",
    "          row[column_id] = column_text\n",
    "          dynamic_columns.add(column_id)\n",
    "          print(\"Specific Col ID\", col.get(\"id\"))\n",
    "          print(\"Specific Col Value\", col.get(\"text\"))\n",
    "\n",
    "        all_items.append(row)\n",
    "\n",
    "    # Check for next page\n",
    "    cursor = items_page[\"cursor\"]\n",
    "    if not cursor:\n",
    "        break  # No more data, exit loop\n",
    "\n",
    "# Convert list to Pandas DataFrame\n",
    "df = pd.DataFrame(all_items)\n",
    "\n",
    "df = df.astype(str)  # Convert all columns to string type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c078986-6b63-423b-aa0b-fd2fb7e53136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Display DataFrame in Databricks\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "if df.empty:\n",
    "    print(\"DataFrame is empty. Skipping Spark conversion.\")\n",
    "else:\n",
    "    # Define schema based on expected data types\n",
    "    schema = StructType([\n",
    "      StructField(\"Board ID\", StringType(), True),\n",
    "      StructField(\"Board Name\", StringType(), True),\n",
    "      StructField(\"Item ID\", StringType(), True),\n",
    "      StructField(\"Item Name\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Add dynamic columns to schema\n",
    "    for col in dynamic_columns:\n",
    "      schema.add(StructField(col, StringType(), True))\n",
    "\n",
    "    # Convert Pandas DF to a list of dictionaries\n",
    "    data_records = df.astype(str).to_dict(orient=\"records\")  \n",
    "\n",
    "    # Convert list of dicts to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(data_records, schema=schema)\n",
    "    \n",
    "    display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af774ff-6e5f-43f1-b0d0-5d241034d4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Store Data in a Temporary Table\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a temporary view\n",
    "spark_df.createOrReplaceTempView(\"monday_temp_table\")\n",
    "\n",
    "## For PowerBI make use of a Table stored in an ADLS location for PropertyTech\n",
    "## spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"path\",\"abfss://corporate-services@dtaeunlabadlsanalyst01.dfs.core.windows.net/ propertytech_azlab_execute.db/highly_sensitive/monday_temp_table\").saveAsTable(\"propertytech_azlab_execute.db.monday_temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79b6a68e-fda7-4610-8d02-8a6ce8f0aa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read in an excel file containing the last week's clash report\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1788156532260850,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "POC Weekly Clash Report Updates 2025-03-14",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
